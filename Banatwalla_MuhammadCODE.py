# -*- coding: utf-8 -*-
"""HW3_6373.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h8j7ClUCxz8dhm_0SqsXsb_FozKYVYb2

**Start:Downloading required packages**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import io
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, Dropout
from tensorflow.keras.utils import plot_model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import losses, optimizers
from tensorflow.keras import callbacks
from tensorflow.keras.models import load_model
from google.colab import files
from PIL import Image as im

"""**End : Downloading required Packages**

#**Step 1 & 2: Import Data and Reshape for CNN**

**Start: Import Data**
"""

uploaded=files.upload()

"""**End: Import Data**

**Start: Reading the data and splitting into test and train**
"""

CENTURY = pd.read_csv('CENTURY.csv')
EBRIMA = pd.read_csv('EBRIMA.csv')
GILL = pd.read_csv('GILL.csv')
PROXY = pd.read_csv('PROXY.csv')
LEELAWADEE = pd.read_csv('LEELAWADEE.csv')
###Remove Unwanted Columns###
CENTURY = CENTURY.drop(columns= ['fontVariant','m_label','orientation','m_top','m_left','originalH','originalW','h','w'])
EBRIMA = EBRIMA.drop(columns= ['fontVariant','m_label','orientation','m_top','m_left','originalH','originalW','h','w'])
GILL = GILL.drop(columns= ['fontVariant','m_label','orientation','m_top','m_left','originalH','originalW','h','w'])
PROXY = PROXY.drop(columns= ['fontVariant','m_label','orientation','m_top','m_left','originalH','originalW','h','w'])
LEELAWADEE = LEELAWADEE.drop(columns= ['fontVariant','m_label','orientation','m_top','m_left','originalH','originalW','h','w'])
#Three classes of images of "normal" characters
CL1 = CENTURY[(CENTURY.strength == .4) & (CENTURY.italic == 0)]
CL2 = EBRIMA[(EBRIMA.strength == .4) & (EBRIMA.italic == 0)]
CL3 = GILL[(GILL.strength == .4) & (GILL.italic == 0)]
CL4 = PROXY[(PROXY.strength == .4) & (PROXY.italic == 0)]
CL5 = LEELAWADEE[(LEELAWADEE.strength == .4) & (LEELAWADEE.italic == 0)]

#Train Test split for CL1
CL1_train, CL1_test = train_test_split(CL1,test_size=0.2,random_state=101)
#Train Test split for CL2
CL2_train, CL2_test = train_test_split(CL2,test_size=0.2,random_state=101)
#Train Test split for CL3
CL3_train, CL3_test = train_test_split(CL3,test_size=0.2,random_state=101)
#Train Test split for CL4
CL4_train, CL4_test = train_test_split(CL4,test_size=0.2,random_state=101)
#Train Test split for CL5
CL5_train, CL5_test = train_test_split(CL5,test_size=0.2,random_state=101)

#Full training set and test set
X_train = pd.concat([CL1_train,CL2_train,CL3_train, CL4_train, CL5_train])
X_test = pd.concat([CL1_test,CL2_test,CL3_test, CL4_test, CL5_test])
#need to remove font, strength, and italic column
Y_train = X_train['font']
Y_test = X_test['font']
#need to remove font, strength, and italic column
X_train = X_train.drop(columns= ['font','strength','italic'])
X_test = X_test.drop(columns = ['font','strength','italic'])
X = pd.concat([X_train,X_test])
CL1 = CL1.drop(columns = ['font','strength','italic'])
CL2 = CL2.drop(columns = ['font','strength','italic'])
CL3 = CL3.drop(columns = ['font','strength','italic'])
CL4 = CL4.drop(columns = ['font','strength','italic'])
CL5 = CL5.drop(columns = ['font','strength','italic'])
#convert Y_train and Y_test to categorical vectors
Y_train = pd.Categorical(pd.factorize(Y_train)[0])
Y_train = pd.DataFrame(Y_train)
Y_test = pd.Categorical(pd.factorize(Y_test)[0])
Y_test = pd.DataFrame(Y_test)
num_classes = 5
Y_train = tf.keras.utils.to_categorical(Y_train, num_classes)
Y_test = tf.keras.utils.to_categorical(Y_test, num_classes)
print(X_test.shape)
print(Y_test.shape)

"""**End: Reading The data and splitting into training and testing set**

**Start: Reshape the data**
"""

#reshape the input data
X_train = X_train.to_numpy().reshape(6671,20,20,1)
X_test = X_test.to_numpy().reshape(1670,20,20,1)

X_train.shape

"""**End: Reshape the data**

**Start: Print and Save 'A' image for each font**
"""

#CL1 Image
row1_CL1 = CL1.loc[[0]]
array_CL1 = np.array(row1_CL1, dtype=np.uint8)
array_CL1 = np.reshape(array_CL1, (20, 20))
data_CL1 = im.fromarray(array_CL1)
#now data is our image and we can save it
#newImg1.save("img1.png","PNG")
data_CL1.save("Centuryimg.png","PNG")
plt.imshow(data_CL1)

#CL2 Image
row1_CL2 = CL2.loc[[2]]
array_CL2 = np.array(row1_CL2, dtype=np.uint8)
array_CL2 = np.reshape(array_CL2, (20, 20))
data_CL2 = im.fromarray(array_CL2)
#now data is our image and we can save it
data_CL2.save("Ebrimaimg.png","PNG")
plt.imshow(data_CL2)
#data_CL2.save(‘CL2_A.png’)

#CL3 Image
row1_CL3 = CL3.loc[[0]]
array_CL3 = np.array(row1_CL3, dtype=np.uint8)
array_CL3 = np.reshape(array_CL3, (20, 20))
data_CL3 = im.fromarray(array_CL3)
#now data is our image and we can save it
plt.imshow(data_CL3)
data_CL3.save("Gillimg.png","PNG")
#data_CL3.save(‘CL3_A.png’)

#CL4 Image 
#133
row1_CL4 = CL4.loc[[165]]
array_CL4 = np.array(row1_CL4, dtype=np.uint8)
array_CL4 = np.reshape(array_CL4, (20, 20))
data_CL4 = im.fromarray(array_CL4)
#now data is our image and we can save it
plt.imshow(data_CL4)
data_CL4.save("Proxyimg.png","PNG")
#data_CL4.save(‘CL4_A.png’)

#CL5 Image
row1_CL5 = CL5.loc[[0]]
array_CL5 = np.array(row1_CL5, dtype=np.uint8)
array_CL5 = np.reshape(array_CL5, (20, 20))
data_CL5 = im.fromarray(array_CL5)
#now data is our image and we can save it
plt.imshow(data_CL5)
data_CL5.save("Leelawadeeimg.png","PNG")
#data_CL5.save(‘CL5_A.png’)

"""**Start: Print and Save 'A' image for each font**

**Start: Normalizing Data**
"""

X_train = X_train/255
X_test = X_test/255

"""**End: Normalizing Data**

# **STEP 3-5: Implement CNN Arcitecture**

***Start: CNN with h = 90**

I haven't implemented Dropout or early stopping, hopefully TA will show us code for that
"""

model = Sequential()
model.add(Conv2D(16, (5, 5),padding = 'valid',  input_shape=(20,20,1), strides= 1))

model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))
#model.add(Dropout(.5))

model.add(Conv2D(16, (3, 3), strides = 1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))
#model.add(Dropout(.5))

model.add(Flatten())
model.add(Dense(90, activation='relu', )) #hidden layer 
model.add(Dropout(.5))
model.add(Dense(5))
model.add(Activation('softmax'))

model.summary()

#can adjust these parameters
from tensorflow.keras.optimizers import Adam

opt = Adam(lr=0.001, decay=1e-7)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

#can change the file name to whatever we want

from tensorflow.keras.callbacks import ModelCheckpoint
checkpointer = ModelCheckpoint(filepath='/content/weights.hdf5', monitor='val_accuracy', save_best_only=True)
es=EarlyStopping(monitor='val_loss',mode="min",patience=25)

#can play with epochs
Monitor = model.fit(X_train, Y_train,
              batch_size=77,
              epochs=600,
              validation_data=(X_test, Y_test),
              callbacks = [checkpointer,es],
              shuffle = True)

"""add plots and confusion matrices here

**End: CNN with h = 90**
"""



"""**Start cross entropy plots for h=90**"""

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(101)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,101])
plt.ylim([0,1.45])
plt.savefig('CNN90_CrossEntropy.png')

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,6))
fig.suptitle('')
x1=range(0,45)
x2 = range(45,83)
#x3= range(100,150)
#x4 = range(150,205)
train_loss1=train_loss[0:45]
train_loss2=train_loss[45:83]
#train_loss3=train_loss[100:150]
#train_loss4=train_loss[150:205]
val_loss1=val_loss[0:45]
val_loss2=val_loss[45:83]
#val_loss3=val_loss[100:150]
#val_loss4=val_loss[150:205]
ax1.plot(x1, train_loss1)
ax1.plot(x1, val_loss1)
ax2.plot(x2, train_loss2)
ax2.plot(x2, val_loss2)
#ax3.plot(x3, train_loss3)
#ax3.plot(x3, val_loss3)
#ax4.plot(x4, train_loss4)
#ax4.plot(x4,val_loss4)

xc=range(83)
plt.savefig('CNN90_crossentropy.png')

"""**End: Cross entropy plots for h=90**

**Start Accuracy plots for h=90**
"""

#Visualize accuracy
train_loss=Monitor.history['accuracy']
val_loss=Monitor.history['val_accuracy']
xc=range(101)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.xlim([0,101])
plt.ylim([0.5,1])
plt.savefig('CNNh90_accuracy.png')

"""**End:Cross Accuracy plots for h=90**

**Start:confusion matrix for h=90**
"""

Y_Pred=model.predict_classes(X_test)
Y_Pred_hot=[]
for i in range(len(Y_Pred)):
  if Y_Pred[i]==0:
    new_list=[1,0,0,0,0]
  elif Y_Pred[i]==1:
    new_list=[0,1,0,0,0]
  elif Y_Pred[i]==2:
    new_list=[0,0,1,0,0]
  elif Y_Pred[i]==3:
    new_list=[0,0,0,1,0]
  else:
    new_list=[0,0,0,0,1]
  Y_Pred_hot.append(new_list)
  i=i+1

Y_Pred_array=np.array(Y_Pred_hot)

from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))

"""**End: Confusion Matrices for h=90**

**Start:add to the list of accuracy for h90**
"""

#Computes accuracy of best model 
best_modelh2 =load_model('weights.hdf5')
best_modelh2.evaluate(X_test,Y_test)

# creates an empty list for accuracys and appends the accuracy of current model on it, will use for final sketch to compare best approach
allacc=[]
modelaccs=[]
trainaccs=[]
x=max(val_loss)#also gets accuracy of best model 
y="H90"
z=train_loss[val_loss.index(x)]
trainaccs.append(z)
allacc.append(x)
modelaccs.append(y)

allacc

"""**End:add to the list of accuracy for h90**

**start: CNN with h=150**
"""

model = Sequential()
model.add(Conv2D(16, (5, 5), input_shape=(20,20,1), strides= 1))

model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Conv2D(16, (3, 3), padding='same', strides = 1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Flatten())
model.add(Dense(150, activation='relu', )) #hidden layer 
model.add(Dropout(.5))
model.add(Dense(5))
model.add(Activation('softmax'))

model.summary()

#can adjust these parameters
from tensorflow.keras.optimizers import Adam

opt = Adam(lr=0.001, decay=1e-7)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

#can change the file name to whatever we want

from tensorflow.keras.callbacks import ModelCheckpoint
checkpointer = ModelCheckpoint(filepath='/content/weights.hdf5', monitor='val_accuracy', save_best_only=True)
es=EarlyStopping(monitor='val_loss',mode="min",patience=25)

#can play with epochs
Monitor = model.fit(X_train, Y_train,
              batch_size=77,
              epochs=600,
              validation_data=(X_test, Y_test),
              callbacks = [checkpointer,es],
              shuffle = True)

"""**End: CNN with h=150**

**Start cross entropy plots for h=150**
"""

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(83)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,83])
plt.ylim([0,2])
plt.savefig('MLP0_CrossEntropy.png')

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,6))
fig.suptitle('')
x1=range(0,31)
x2 = range(31,62)
train_loss1=train_loss[0:31]
train_loss2=train_loss[31:62]
val_loss1=val_loss[0:31]
val_loss2=val_loss[31:62]
ax1.plot(x1, train_loss1)
ax1.plot(x1, val_loss1)
ax2.plot(x2, train_loss2)
ax2.plot(x2, val_loss2)

xc=range(62)
plt.savefig('CNN150_crossentropy.png')

"""**End:Cross entropy plots for h=150**

**Start: Accuracy plots for h=150**
"""

#Visualize accuracy
train_loss=Monitor.history['accuracy']
val_loss=Monitor.history['val_accuracy']
xc=range(83)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.xlim([0,83])
plt.ylim([0.5,1])
plt.savefig('CNNh150_Accuracy.png')

"""**End:Accuracy plots for h=150**

**Start:confusion matrix for h=150**
"""

Y_Pred=model.predict_classes(X_test)
Y_Pred_hot=[]
for i in range(len(Y_Pred)):
  if Y_Pred[i]==0:
    new_list=[1,0,0,0,0]
  elif Y_Pred[i]==1:
    new_list=[0,1,0,0,0]
  elif Y_Pred[i]==2:
    new_list=[0,0,1,0,0]
  elif Y_Pred[i]==3:
    new_list=[0,0,0,1,0]
  else:
    new_list=[0,0,0,0,1]
  Y_Pred_hot.append(new_list)
  i=i+1

Y_Pred_array=np.array(Y_Pred_hot)

from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))

"""**End: Confusion Matrices for h=150**

**Start:add to the list of accuracy for h150**

> Indented block
"""

#Computes accuracy of best model 
best_modelh2 =load_model('weights.hdf5')
best_modelh2.evaluate(X_test,Y_test)

# creates an empty list for accuracys and appends the accuracy of current model on it, will use for final sketch to compare best approach
x=max(val_loss)#also gets accuracy of best model 
y="H150"
z=train_loss[val_loss.index(x)]
trainaccs.append(z)
allacc.append(x)
modelaccs.append(y)

print(allacc,modelaccs,trainaccs)

"""**END:add to the list of accuracy for h150**

> Indented block

**Start:CNN with h=200**
"""

model = Sequential()
model.add(Conv2D(16, (5, 5), input_shape=(20,20,1), strides= 1))

model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Conv2D(16, (3, 3), padding='same', strides = 1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Flatten())
model.add(Dense(200, activation='relu', )) #hidden layer 
model.add(Dropout(.5))
model.add(Dense(5))
model.add(Activation('softmax'))

model.summary()

#can adjust these parameters
from tensorflow.keras.optimizers import Adam

opt = Adam(lr=0.001, decay=1e-7)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

#can change the file name to whatever we want

from tensorflow.keras.callbacks import ModelCheckpoint
checkpointer = ModelCheckpoint(filepath='/content/weights.hdf5', monitor='val_accuracy', save_best_only=True)
es=EarlyStopping(monitor='val_loss',mode="min",patience=25)

#can play with epochs
Monitor = model.fit(X_train, Y_train,
              batch_size=77,
              epochs=600,
              validation_data=(X_test, Y_test),
              callbacks = [checkpointer,es],
              shuffle = True)

"""**Start cross entropy plots for h=200**"""

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(60)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,60])
plt.ylim([0,2])
plt.savefig('MLP0_CrossEntropy.png')

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,6))
fig.suptitle('')
x1=range(0,28)
x2 = range(28,56)
train_loss1=train_loss[0:28]
train_loss2=train_loss[28:56]
val_loss1=val_loss[0:28]
val_loss2=val_loss[28:56]
ax1.plot(x1, train_loss1)
ax1.plot(x1, val_loss1)
ax2.plot(x2, train_loss2)
ax2.plot(x2, val_loss2)

xc=range(56)
plt.savefig('CNN200_crossentropy.png')

"""**End:Cross entropy plots for h=200**

**Start:Accuracy plot for h=200**
"""

#Visualize accuracy
train_loss=Monitor.history['accuracy']
val_loss=Monitor.history['val_accuracy']
xc=range(60)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.xlim([0,60])
plt.ylim([0.5,0.95])
plt.savefig('CNNh200_Accuracy.png')

"""**End:Accuracy plots for h=200**

**Start:confusion matrix for h=200**
"""

Y_Pred=model.predict_classes(X_test)
Y_Pred_hot=[]
for i in range(len(Y_Pred)):
  if Y_Pred[i]==0:
    new_list=[1,0,0,0,0]
  elif Y_Pred[i]==1:
    new_list=[0,1,0,0,0]
  elif Y_Pred[i]==2:
    new_list=[0,0,1,0,0]
  elif Y_Pred[i]==3:
    new_list=[0,0,0,1,0]
  else:
    new_list=[0,0,0,0,1]
  Y_Pred_hot.append(new_list)
  i=i+1

Y_Pred_array=np.array(Y_Pred_hot)

from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))

"""**End: Confusion Matrices for h=200**

**Start:add to the list of accuracy for h200**
"""

#Computes accuracy of best model 
best_modelh2 =load_model('weights.hdf5')
best_modelh2.evaluate(X_test,Y_test)

# creates an empty list for accuracys and appends the accuracy of current model on it, will use for final sketch to compare best approach
x=max(val_loss)#also gets accuracy of best model 
y="H200"
z=train_loss[val_loss.index(x)]
trainaccs.append(z)
allacc.append(x)
modelaccs.append(y)

print(allacc,modelaccs)

"""**END:add to the list of accuracy for h200**



"""



"""**Start:CNN with h=250**

"""

model = Sequential()
model.add(Conv2D(16, (5, 5), input_shape=(20,20,1), strides= 1))

model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Conv2D(16, (3, 3), padding='same', strides = 1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Flatten())
model.add(Dense(250, activation='relu', )) #hidden layer 
model.add(Dropout(.5))
model.add(Dense(5))
model.add(Activation('softmax'))

model.summary()

#can adjust these parameters
from tensorflow.keras.optimizers import Adam

opt = Adam(lr=0.001, decay=1e-7)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

#can change the file name to whatever we want

from tensorflow.keras.callbacks import ModelCheckpoint
checkpointer = ModelCheckpoint(filepath='/content/weights.hdf5', monitor='val_accuracy', save_best_only=True)
es=EarlyStopping(monitor='val_loss',mode="min",patience=25)

#can play with epochs
Monitor = model.fit(X_train, Y_train,
              batch_size=77,
              epochs=600,
              validation_data=(X_test, Y_test),
              callbacks = [checkpointer,es],
              shuffle = True)

"""**END:MLP with h=250**

**Start cross entropy plots for h=250**
"""

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(62)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,62])
plt.ylim([0,2])
plt.savefig('MLP0_CrossEntropy.png')

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,6))
fig.suptitle('')
x1=range(0,28)
x2 = range(28,56)
train_loss1=train_loss[0:28]
train_loss2=train_loss[28:56]
val_loss1=val_loss[0:28]
val_loss2=val_loss[28:56]
ax1.plot(x1, train_loss1)
ax1.plot(x1, val_loss1)
ax2.plot(x2, train_loss2)
ax2.plot(x2, val_loss2)

xc=range(56)
plt.savefig('CNN250_crossentropy.png')

"""**End:Cross entropy plots for h=250**

**Start:Accuracy plot for h=250**
"""

#Visualize accuracy
train_loss=Monitor.history['accuracy']
val_loss=Monitor.history['val_accuracy']
xc=range(62)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.xlim([0,62])
plt.ylim([0.5,1])
plt.savefig('CNNh250_Accuracy.png')

"""**End:Accuracy plots for h=250**

**Start:confusion matrix for h=250**
"""

Y_Pred=model.predict_classes(X_test)
Y_Pred_hot=[]
for i in range(len(Y_Pred)):
  if Y_Pred[i]==0:
    new_list=[1,0,0,0,0]
  elif Y_Pred[i]==1:
    new_list=[0,1,0,0,0]
  elif Y_Pred[i]==2:
    new_list=[0,0,1,0,0]
  elif Y_Pred[i]==3:
    new_list=[0,0,0,1,0]
  else:
    new_list=[0,0,0,0,1]
  Y_Pred_hot.append(new_list)
  i=i+1

Y_Pred_array=np.array(Y_Pred_hot)

from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))

"""**End: Confusion Matrices for h=250**

**start:Adding accuracy for model H250**
"""

#Computes accuracy of best model 
best_modelh2 =load_model('weights.hdf5')
best_modelh2.evaluate(X_test,Y_test)

# creates an empty list for accuracys and appends the accuracy of current model on it, will use for final sketch to compare best approach
x=max(val_loss)#also gets accuracy of best model 
y="H250"
allacc.append(x)
modelaccs.append(y)
z=train_loss[val_loss.index(x)]
trainaccs.append(z)

print(allacc,modelaccs)

"""**End:Adding accuracy for model H250**

**Start:CNN with h=300**
"""

model = Sequential()
model.add(Conv2D(16, (5, 5), input_shape=(20,20,1), strides= 1))

model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Conv2D(16, (3, 3), padding='same', strides = 1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Flatten())
model.add(Dense(300, activation='relu', )) #hidden layer 
model.add(Dropout(.5))
model.add(Dense(5))
model.add(Activation('softmax'))

model.summary()

#can adjust these parameters
from tensorflow.keras.optimizers import Adam

opt = Adam(lr=0.001, decay=1e-7)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

#can change the file name to whatever we want

from tensorflow.keras.callbacks import ModelCheckpoint
checkpointer = ModelCheckpoint(filepath='/content/weights.hdf5', monitor='val_accuracy', save_best_only=True)
es=EarlyStopping(monitor='val_loss',mode="min",patience=25)

#can play with epochs
Monitor = model.fit(X_train, Y_train,
              batch_size=77,
              epochs=600,
              validation_data=(X_test, Y_test),
              callbacks = [checkpointer,es],
              shuffle = True)

"""**END:MLP with h=300**

**Start cross entropy plots for h=300**
"""

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(57)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,57])
plt.ylim([0,2])
plt.savefig('MLP0_CrossEntropy.png')

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,6))
fig.suptitle('')
x1=range(0,35)
x2 = range(35,69)
train_loss1=train_loss[0:35]
train_loss2=train_loss[35:69]
val_loss1=val_loss[0:35]
val_loss2=val_loss[35:69]
ax1.plot(x1, train_loss1)
ax1.plot(x1, val_loss1)
ax2.plot(x2, train_loss2)
ax2.plot(x2, val_loss2)

xc=range(69)
plt.savefig('CNN300_crossentropy.png')

"""**End:Cross entropy plots for h=300**

**Start:Accuracy plot for h=300**
"""

#Visualize accuracy
train_loss=Monitor.history['accuracy']
val_loss=Monitor.history['val_accuracy']
xc=range(57)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.xlim([0,57])
plt.ylim([0.5,1])
plt.savefig('CNNh300_Accuracy.png')

"""**End:Accuracy plots for h=300**

**Start:confusion matrix for h=300**
"""

Y_Pred=model.predict_classes(X_test)
Y_Pred_hot=[]
for i in range(len(Y_Pred)):
  if Y_Pred[i]==0:
    new_list=[1,0,0,0,0]
  elif Y_Pred[i]==1:
    new_list=[0,1,0,0,0]
  elif Y_Pred[i]==2:
    new_list=[0,0,1,0,0]
  elif Y_Pred[i]==3:
    new_list=[0,0,0,1,0]
  else:
    new_list=[0,0,0,0,1]
  Y_Pred_hot.append(new_list)
  i=i+1

Y_Pred_array=np.array(Y_Pred_hot)

from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))

"""**End: Confusion Matrices for h=300**

**start:Adding accuracy for model H300**
"""

#Computes accuracy of best model 
best_modelh2 =load_model('weights.hdf5')
best_modelh2.evaluate(X_test,Y_test)

# creates an empty list for accuracys and appends the accuracy of current model on it, will use for final sketch to compare best approach
x=max(val_loss)#also gets accuracy of best model 
y="H300"
allacc.append(x)
modelaccs.append(y)
z=train_loss[val_loss.index(x)]
trainaccs.append(z)

print(allacc,modelaccs)

"""**End:Adding accuracy for model H300**

**Start:CNN with h=400**
"""

model = Sequential()
model.add(Conv2D(16, (5, 5), input_shape=(20,20,1), strides= 1))

model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Conv2D(16, (3, 3), padding='same', strides = 1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Flatten())
model.add(Dense(400, activation='relu', )) #hidden layer 
model.add(Dropout(.5))
model.add(Dense(5))
model.add(Activation('softmax'))

model.summary()

#can adjust these parameters
from tensorflow.keras.optimizers import Adam

opt = Adam(lr=0.001, decay=1e-7)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

#can change the file name to whatever we want

from tensorflow.keras.callbacks import ModelCheckpoint
checkpointer = ModelCheckpoint(filepath='/content/weights.hdf5', monitor='val_accuracy', save_best_only=True)
es=EarlyStopping(monitor='val_loss',mode="min",patience=25)

#can play with epochs
Monitor = model.fit(X_train, Y_train,
              batch_size=77,
              epochs=600,
              validation_data=(X_test, Y_test),
              callbacks = [checkpointer,es],
              shuffle = True)

"""**END:MLP with h=400**

**Start cross entropy plots for h=400**
"""

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(62)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,62])
plt.ylim([0,2])
plt.savefig('MLP0_CrossEntropy.png')

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,6))
fig.suptitle('')
x1=range(0,30)
x2 = range(30,55)
train_loss1=train_loss[0:30]
train_loss2=train_loss[30:55]
val_loss1=val_loss[0:30]
val_loss2=val_loss[30:55]
ax1.plot(x1, train_loss1)
ax1.plot(x1, val_loss1)
ax2.plot(x2, train_loss2)
ax2.plot(x2, val_loss2)

xc=range(55)
plt.savefig('CNN400_crossentropy.png')

"""**End:Cross entropy plots for h=400**

**Start:Accuracy plot for h=400**
"""

#Visualize accuracy
train_loss=Monitor.history['accuracy']
val_loss=Monitor.history['val_accuracy']
xc=range(62)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.xlim([0,62])
plt.ylim([0.5,1])
plt.savefig('CNNh400_Accuracy.png')

"""**End:Accuracy plots for h=400**

**Start:confusion matrix for h=400**
"""

Y_Pred=model.predict_classes(X_test)
Y_Pred_hot=[]
for i in range(len(Y_Pred)):
  if Y_Pred[i]==0:
    new_list=[1,0,0,0,0]
  elif Y_Pred[i]==1:
    new_list=[0,1,0,0,0]
  elif Y_Pred[i]==2:
    new_list=[0,0,1,0,0]
  elif Y_Pred[i]==3:
    new_list=[0,0,0,1,0]
  else:
    new_list=[0,0,0,0,1]
  Y_Pred_hot.append(new_list)
  i=i+1

Y_Pred_array=np.array(Y_Pred_hot)

from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))

"""**End: Confusion Matrices for h=400**"""

#Computes accuracy of best model 
best_modelh2 =load_model('weights.hdf5')
best_modelh2.evaluate(X_test,Y_test)

# creates an empty list for accuracys and appends the accuracy of current model on it, will use for final sketch to compare best approach
x=max(val_loss)#also gets accuracy of best model 
y="H400"
allacc.append(x)
modelaccs.append(y)
z=train_loss[val_loss.index(x)]
trainaccs.append(z)

print(allacc,modelaccs)

"""**Start:CNN with h=500**

"""

model = Sequential()
model.add(Conv2D(16, (5, 5), input_shape=(20,20,1), strides= 1))

model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Conv2D(16, (3, 3), padding='same', strides = 1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))

model.add(Flatten())
model.add(Dense(500, activation='relu', )) #hidden layer 
model.add(Dropout(.5))
model.add(Dense(5))
model.add(Activation('softmax'))

model.summary()

#can adjust these parameters
from tensorflow.keras.optimizers import Adam

opt = Adam(lr=0.001, decay=1e-7)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

#can change the file name to whatever we want

from tensorflow.keras.callbacks import ModelCheckpoint
checkpointer = ModelCheckpoint(filepath='/content/weights.hdf5', monitor='val_accuracy', save_best_only=True)
es=EarlyStopping(monitor='val_loss',mode="min",patience=25)

#can play with epochs
Monitor = model.fit(X_train, Y_train,
              batch_size=77,
              epochs=600,
              validation_data=(X_test, Y_test),
              callbacks = [checkpointer,es],
              shuffle = True)

"""**END:MLP with h=500**

**Start cross entropy plots for h=500**
"""

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(61)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,61])
plt.ylim([0,2])
plt.savefig('MLP0_CrossEntropy.png')

fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(20,6))
fig.suptitle('')
x1=range(0,25)
x2 = range(25,51)
train_loss1=train_loss[0:25]
train_loss2=train_loss[25:51]
val_loss1=val_loss[0:25]
val_loss2=val_loss[25:51]
ax1.plot(x1, train_loss1)
ax1.plot(x1, val_loss1)
ax2.plot(x2, train_loss2)
ax2.plot(x2, val_loss2)

xc=range(51)
plt.savefig('CNN500_crossentropy.png')

"""**End:Cross entropy plots for h=500**

**Start:Accuracy plot for h=500**
"""

#Visualize accuracy
train_loss=Monitor.history['accuracy']
val_loss=Monitor.history['val_accuracy']
xc=range(61)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.xlim([0,61])
plt.ylim([0.5,1])
plt.savefig('CNNh500_Accuracy.png')

"""**End:Accuracy plots for h=500**

**Start:confusion matrix for h=500**
"""

Y_Pred=model.predict_classes(X_test)
Y_Pred_hot=[]
for i in range(len(Y_Pred)):
  if Y_Pred[i]==0:
    new_list=[1,0,0,0,0]
  elif Y_Pred[i]==1:
    new_list=[0,1,0,0,0]
  elif Y_Pred[i]==2:
    new_list=[0,0,1,0,0]
  elif Y_Pred[i]==3:
    new_list=[0,0,0,1,0]
  else:
    new_list=[0,0,0,0,1]
  Y_Pred_hot.append(new_list)
  i=i+1

Y_Pred_array=np.array(Y_Pred_hot)

from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))

"""**End: Confusion Matrices for h=500**"""

#Computes accuracy of best model 
best_modelh2 =load_model('weights.hdf5')
best_modelh2.evaluate(X_test,Y_test)

# creates an empty list for accuracys and appends the accuracy of current model on it, will use for final sketch to compare best approach
x=max(val_loss)#also gets accuracy of best model 
y="H500"
allacc.append(x)
modelaccs.append(y)
z=train_loss[val_loss.index(x)]
trainaccs.append(z)

print(allacc,modelaccs)

print(trainaccs)

"""**Plots for accuracy Performance**"""

#Visualize accuracy performance

xc=range(7)
plt.figure(1,figsize=(7,5))
plt.plot(xc,allacc,label='Test')
plt.plot(xc,trainaccs,label="Train")
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.legend()
plt.xticks(xc,modelaccs)
plt.ylim([0.7,1])
plt.savefig('CNN_All Accuracy.png')

"""**End: Plots for Accuracy performances**

##**VVVV further improvements down here**

**Start:CNN with h1=500 and h2=250**
"""

model = Sequential()
model.add(Conv2D(16, (5, 5), input_shape=(20,20,1), strides= 1))

model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))
#model.add(Dropout(.5))

model.add(Conv2D(16, (3, 3), padding='same', strides = 1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))
#model.add(Dropout(.5))

model.add(Flatten())
model.add(Dense(500, activation='relu', )) #hidden layer 1
model.add(Dropout(.5))
model.add(Dense(250, activation='relu', )) #hidden layer 2
model.add(Dropout(.5))
model.add(Dense(125, activation='relu', )) #hidden layer 3
model.add(Dropout(.5))
model.add(Dense(65, activation='relu', )) #hidden layer 3
model.add(Dropout(.5))
model.add(Dense(5))
model.add(Activation('softmax'))

model.summary()

#can adjust these parameters
from tensorflow.keras.optimizers import Adam

opt = Adam(lr=0.001, decay=1e-7)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

#can change the file name to whatever we want

from tensorflow.keras.callbacks import ModelCheckpoint
checkpointer = ModelCheckpoint(filepath='/content/weights.hdf5', monitor='val_accuracy', save_best_only=True)
es=EarlyStopping(monitor='val_loss',mode="min",patience=25)

#can play with epochs
Monitor = model.fit(X_train, Y_train,
              batch_size=25,
              epochs=600,
              validation_data=(X_test, Y_test),
              callbacks = [checkpointer,es],
              shuffle = True)

"""**END:MLP with h1=500 and h2 =250**

**Start cross entropy plots for h1=500 and h2=250**
"""

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(47)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,47])
plt.ylim([0,2])
plt.savefig('MLP0_CrossEntropy.png')

"""**End:Cross entropy plots for h1=500 and h2=250**

**Start:Accuracy plot for h1=500 and h2=250**
"""

#Visualize accuracy
train_loss=Monitor.history['accuracy']
val_loss=Monitor.history['val_accuracy']
xc=range(47)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.xlim([0,47])
plt.ylim([0.5,1])
plt.savefig('MLP0_CrossEntropy.png')

"""**End:Accuracy plots for h1=500 and h2=250**

**Start:confusion matrix for h1=500 and h2=250**
"""

Y_Pred=model.predict_classes(X_test)
Y_Pred_hot=[]
for i in range(len(Y_Pred)):
  if Y_Pred[i]==0:
    new_list=[1,0,0,0,0]
  elif Y_Pred[i]==1:
    new_list=[0,1,0,0,0]
  elif Y_Pred[i]==2:
    new_list=[0,0,1,0,0]
  elif Y_Pred[i]==3:
    new_list=[0,0,0,1,0]
  else:
    new_list=[0,0,0,0,1]
  Y_Pred_hot.append(new_list)
  i=i+1

Y_Pred_array=np.array(Y_Pred_hot)

from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))



#Computes accuracy of best model 
best_modelh2 =load_model('weights.hdf5')
best_modelh2.evaluate(X_test,Y_test)

max(val_loss)

"""**End: Confusion Matrices for h1=500 and h2=250**

https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html

**^^parking this here for the moment, will use it for further suggestions**

**Playing with batch normalization, h1=500 h2=250**
"""



from keras.layers import BatchNormalization
model = Sequential()
model.add(Conv2D(16, (5, 5), input_shape=(20,20,1), strides= 1))

model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))
model.add(Dropout(.5))

model.add(Conv2D(16, (3, 3), padding='same', strides = 1))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2), strides = 2))
model.add(BatchNormalization())

model.add(Flatten())
model.add(Dense(500, activation='relu', )) #hidden layer 1
model.add(Dropout(0.5))
model.add(Dense(250, activation='relu', )) #hidden layer 2
model.add(Dropout(0.5))
model.add(Dense(5))
model.add(Activation('softmax'))

model.summary()

#can adjust these parameters
from tensorflow.keras.optimizers import Adam

opt = Adam(lr=0.001, decay=1e-7)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

#can change the file name to whatever we want

from tensorflow.keras.callbacks import ModelCheckpoint
checkpointer = ModelCheckpoint(filepath='/content/weights.hdf5', monitor='val_accuracy', save_best_only=True)
es=EarlyStopping(monitor='val_accuracy',mode="max",patience=25)

#can play with epochs
Monitor = model.fit(X_train, Y_train,
              batch_size=77,
              epochs=600,
              validation_data=(X_test, Y_test),
              callbacks = [checkpointer,es],
              shuffle = True)



"""**END:MLP with batch normalization h1=500 and h2 =250**

**Start cross entropy plots for h1=500 and h2=250**
"""

#Visualize Cross Entropy
train_loss=Monitor.history['loss']
val_loss=Monitor.history['val_loss']
xc=range(600)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Cross Entropy')
plt.legend()
plt.xlim([0,600])
plt.ylim([0,2])
plt.savefig('MLP0_CrossEntropy.png')

"""**End:Cross entropy plots for h1=500 and h2=250**

**Start:Accuracy plot for h1=500 and h2=250**
"""

#Visualize accuracy
train_loss=Monitor.history['accuracy']
val_loss=Monitor.history['val_accuracy']
xc=range(600)
plt.figure(1,figsize=(7,5))
plt.plot(xc,train_loss, label = 'Train')
plt.plot(xc,val_loss, label = 'Test')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.xlim([0,600])
plt.ylim([0.5,1])
plt.savefig('MLP0_CrossEntropy.png')

"""**End:Accuracy plots for h1=500 and h2=250**

**Start:confusion matrix for h1=500 and h2=250**
"""

Y_Pred=model.predict_classes(X_test)
Y_Pred_hot=[]
for i in range(len(Y_Pred)):
  if Y_Pred[i]==0:
    new_list=[1,0,0,0,0]
  elif Y_Pred[i]==1:
    new_list=[0,1,0,0,0]
  elif Y_Pred[i]==2:
    new_list=[0,0,1,0,0]
  elif Y_Pred[i]==3:
    new_list=[0,0,0,1,0]
  else:
    new_list=[0,0,0,0,1]
  Y_Pred_hot.append(new_list)
  i=i+1

Y_Pred_array=np.array(Y_Pred_hot)

from sklearn.metrics import confusion_matrix

confusion_matrix(Y_test.argmax(axis=1), Y_Pred_array.argmax(axis=1))

"""**Get best accuracy for **"""

#Computes accuracy of best model 
best_modelh2 =load_model('weights.hdf5')
best_modelh2.evaluate(X_test,Y_test)
x=max(val_loss)#also gets accuracy of best model 
allacc.append(x)



